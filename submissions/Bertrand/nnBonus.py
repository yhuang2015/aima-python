# Applied Bonuses:

# 1000+ samples --> the MNIST dataset contains 600,000 samples

# 64+ input variables --> every sample in the MNIST dataset is an image of 28*28 pixels, giving me 784 input variables

# a convolution layer and pooling layer will be thought of as a single layer
# +5 for a 3-layer submission that reduces errors by at least 20%
#  --> adding a 3rd conv + pooling layer with a softmax Dense final layer reduced mislabeled points from 60000 to 1528
#  --> a reduction of much greater than 20%

# +5 for a 4-layer submission that reduces errors by at least 20%
#  --> adding a 4th conv + pooling layer and keeping the softmax Dense final layer reduced mislabeled points
#  --> from 1528 to 1184, a 22.5&% reduction in error


# In summary, here are my applied for bonuses, justified above:
# (5) 1000+ samples
# (5) 64+ input variables
# (5) a 3-layer submission1, that reduces errors by at least 20%
# (5) a 4-layer submission1, that reduces errors by at least 20%

